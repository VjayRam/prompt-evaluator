{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692a4655",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pydantic>=2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "692852d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from metrics.eval_templates import EvalMetricTemplates\n",
    "from metrics.eval_metrics import EvalMetric\n",
    "from llms.llm_client import LLMClient\n",
    "from evaluation.eval_engine import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef608ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judge Model Options\n",
    "GEMINI_API_KEY = \"AIzaSyATRXTx0D4rBSL0PUSQFlVCoxVywDo2-os\"\n",
    "OPENAI_API_KEY = \"\"\n",
    "ANTHROPIC_API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b887323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_system_prompt = \"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "838c2e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"datasets/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a93ac87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_model = \"google/gemini-2.0-flash-lite\"\n",
    "\n",
    "if \"openai\" in judge_model:\n",
    "    API_KEY = OPENAI_API_KEY\n",
    "elif \"anthropic\" in judge_model:\n",
    "    API_KEY = ANTHROPIC_API_KEY\n",
    "elif \"google\" in judge_model:\n",
    "    API_KEY = GEMINI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "338663a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        # Instruction\n",
      "        You are an expert evaluator. Your task is to evaluate the quality of the responses generated by AI models.\n",
      "        We will provide you with the user input and an AI-generated responses.\n",
      "        You should first read the user input carefully for analyzing the task, and then evaluate the quality of the responses based on the Criteria provided in the Evaluation section below.\n",
      "        You will assign the response a rating following the Rating Rubric and Evaluation Steps. Give step-by-step explanations for your rating, and only choose ratings from the Rating Rubric.\n",
      "\n",
      "        # Evaluation\n",
      "        ## Metric Definition\n",
      "        You will be assessing coherence, which measures the ability to provide a coherent response based on the user prompt.\n",
      "\n",
      "        ## Criteria\n",
      "        Coherence: A clear and coherent presentation of ideas. The writing should demonstrate\n",
      "        a logical flow, where ideas progress smoothly with clear transitions, and maintain\n",
      "        relevance to the main point. Effective organization is essential, with a clear structure,\n",
      "        signaling, and topic sentences to guide the reader. Additionally, the writing should\n",
      "        exhibit strong cohesion, using word choices, sentence structures, pronouns, and\n",
      "        figurative language to reinforce connections between ideas and create a unified piece.\n",
      "\n",
      "        ## Rating Rubric\n",
      "        5: (Completely coherent). The writing has a seamless logical flow, is expertly organized, and maintains exceptional cohesion throughout.\n",
      "        4: (Mostly coherent). The writing demonstrates strong logical flow, a clear structure, and demonstrates good cohesion.\n",
      "        3: (Somewhat coherent). The writing's logical flow is mostly understandable, it has a recognizable structure, and cohesion is present but could be stronger.\n",
      "        2: (Somewhat incoherent). The writing lacks a clear logical flow, organizational structure is weak, and cohesion is inconsistent or confusing.\n",
      "        1: (Incoherent). The writing is highly illogical, lacks any clear organization, and has little to no cohesion.\n",
      "\n",
      "        ## Evaluation Steps\n",
      "        STEP 1: Identify the purpose and audience: Understanding the writer's goal and intended audience helps determine appropriate coherence expectations.\n",
      "        STEP 2: Assess global flow: Analyze the overall structure and progression of ideas. Does the writing unfold logically, with a clear beginning, middle, and end?\n",
      "        STEP 3: Evaluate local coherence: Examine individual paragraphs and sentence transitions. Are transitions effective in guiding the reader through each point? Do sentences within paragraphs contribute to the main idea?\n",
      "        STEP 4: Analyze word choice and syntax: Look for repetitions, parallelisms, and other rhetorical devices that reinforce connections between ideas. Are they used effectively or confusingly?\n",
      "        STEP 5: Check pronoun and reference clarity: Ensure pronouns and other references are clear and unambiguous, avoiding confusion for the reader.\n",
      "\n",
      "        # User Inputs and AI-generated Response\n",
      "        ## User Inputs\n",
      "        ### Prompt\n",
      "        {prompt}\n",
      "\n",
      "        ## AI-generated Response\n",
      "        {response}\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "coherence = EvalMetricTemplates.PointwiseMetric.COHERENCE\n",
    "print(coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cae6a0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Created!\n",
      "Evaluation Metrics Created!\n",
      "Client initialized: Google Gemini\n",
      "LLM Client Created! Judge Model: google/gemini-2.0-flash-lite\n",
      "Evaluator intialized with metrics: ['multi_turn_chat_quality', 'fluency']\n",
      "Evaluator Created!\n",
      "Number of API calls estimated:  4\n",
      "Evaluation Completed!\n",
      "                       prompt                                     response  \\\n",
      "0                 What is AI?  AI is the simulation of human intelligence.   \n",
      "1  Explain quantum computing.         Quantum computing uses quantum bits.   \n",
      "\n",
      "   multi_turn_chat_quality_rating  \\\n",
      "0                               4   \n",
      "1                               1   \n",
      "\n",
      "                 multi_turn_chat_quality_explanation  fluency_rating  \\\n",
      "0  The response is a concise and accurate definit...               5   \n",
      "1  The response is not collaborative, provides mi...               1   \n",
      "\n",
      "                                 fluency_explanation  \n",
      "0  STEP 1: The response has no grammatical errors...  \n",
      "1  STEP 1: The response consists of only one sent...  \n",
      "{'num_samples': 2, 'multi_turn_chat_quality': {'mean': np.float64(2.5), 'std': np.float64(1.5)}, 'fluency': {'mean': np.float64(3.0), 'std': np.float64(2.0)}}\n"
     ]
    }
   ],
   "source": [
    "# test the entire setup\n",
    "dataset=pd.DataFrame({\n",
    "    \"prompt\": [\"What is AI?\", \"Explain quantum computing.\"],\n",
    "    \"response\": [\"AI is the simulation of human intelligence.\", \"Quantum computing uses quantum bits.\"]\n",
    "})\n",
    "print(\"Dataset Created!\")\n",
    "\n",
    "multi_turn_chat_quality = EvalMetric( metric_name=\"multi_turn_chat_quality\", metric_prompt_template=EvalMetricTemplates.PointwiseMetric.MULTI_TURN_CHAT_QUALITY)\n",
    "fluency = EvalMetric( metric_name=\"fluency\", metric_prompt_template=EvalMetricTemplates.PointwiseMetric.FLUENCY)\n",
    "\n",
    "eval_metrics = [multi_turn_chat_quality, fluency]\n",
    "print(\"Evaluation Metrics Created!\")\n",
    "\n",
    "# Create LLM Client with rate limiting\n",
    "llm_client = LLMClient(\n",
    "    judge_model=judge_model,\n",
    "    api_key=API_KEY,\n",
    "    requests_per_minute=10,  # Custom RPM limit\n",
    "    requests_per_second=1,   # Custom RPS limit\n",
    "    enable_rate_limiting=True\n",
    ")\n",
    "print(\"LLM Client Created! Judge Model:\", llm_client.model)\n",
    "print(\"Rate Limiter Stats:\", llm_client.get_rate_limit_stats())\n",
    "\n",
    "evaluator = Evaluator(llm_client=llm_client, eval_metrics=eval_metrics)\n",
    "print(\"Evaluator Created!\")\n",
    "\n",
    "eval_table, summary = evaluator.evaluate(dataset=dataset)\n",
    "print(\"Evaluation Completed!\")\n",
    "print(eval_table)\n",
    "print(summary)\n",
    "\n",
    "# Show final rate limiter stats\n",
    "print(\"\\nFinal Rate Limiter Stats:\", llm_client.get_rate_limit_stats())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
